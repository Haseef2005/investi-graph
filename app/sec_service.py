import os
import shutil
import glob
from sec_edgar_downloader import Downloader
from bs4 import BeautifulSoup
from app.config import settings
from app import processing, crud, models
from app.database import SessionLocal
import logging
import re

log = logging.getLogger("uvicorn.error")

TEMP_SEC_DIR = "/app/temp_sec" # ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏û‡∏±‡∏Å‡πÑ‡∏ü‡∏•‡πå

def clean_html_content(raw_content: str) -> str:
    """
    1. Extract only the '10-K' document section from the full submission.
    2. Remove HTML tags.
    3. Clean up whitespace.
    """
    if not raw_content:
        return ""

    # --- Step 1: ‡∏´‡∏≤ Document ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å (10-K, 10-Q, 20-F) ---
    # Pattern: ‡∏´‡∏≤ <DOCUMENT> ‡∏ó‡∏µ‡πà‡∏Ç‡πâ‡∏≤‡∏á‡πÉ‡∏ô‡∏°‡∏µ <TYPE>10-K... ‡πÅ‡∏•‡πâ‡∏ß‡∏î‡∏∂‡∏á <TEXT> ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤
    # (?s) ‡∏Ñ‡∏∑‡∏≠‡πÉ‡∏´‡πâ . match newlines ‡πÑ‡∏î‡πâ
    
    # ‡∏•‡∏≠‡∏á‡∏´‡∏≤ 10-K ‡∏´‡∏£‡∏∑‡∏≠ 10-Q ‡∏´‡∏£‡∏∑‡∏≠ 20-F
    doc_match = re.search(
        r'<DOCUMENT>\s*<TYPE>(?:10-K|10-Q|20-F).*?<TEXT>(.*?)</TEXT>\s*</DOCUMENT>', 
        raw_content, 
        re.IGNORECASE | re.DOTALL
    )
    
    if doc_match:
        # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠: ‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô HTML ‡∏Ç‡∏≠‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏°‡∏≤‡πÉ‡∏ä‡πâ (‡∏ó‡∏¥‡πâ‡∏á‡∏Ç‡∏¢‡∏∞‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÑ‡∏õ‡πÄ‡∏•‡∏¢)
        html_content = doc_match.group(1)
    else:
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ pattern (‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå format ‡πÅ‡∏õ‡∏•‡∏Å): ‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡πÅ‡∏ï‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏ß‡∏±‡∏á
        # ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡∏´‡∏≤ tag <TEXT> ‡πÅ‡∏£‡∏Å‡∏™‡∏∏‡∏î‡πÅ‡∏ó‡∏ô ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏°‡∏±‡∏Å‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏´‡∏•‡∏±‡∏Å
        text_match = re.search(r'<TEXT>(.*?)</TEXT>', raw_content, re.IGNORECASE | re.DOTALL)
        if text_match:
            html_content = text_match.group(1)
        else:
            html_content = raw_content # ‡∏à‡∏ô‡∏õ‡∏±‡∏ç‡∏ç‡∏≤ ‡πÉ‡∏ä‡πâ‡∏Ç‡∏≠‡∏á‡πÄ‡∏î‡∏¥‡∏°

    # --- Step 2: BeautifulSoup Cleaning (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°) ---
    soup = BeautifulSoup(html_content, "html.parser")
    
    # ‡∏•‡∏ö Tag ‡∏Ç‡∏¢‡∏∞ (Script, Style, ‡πÅ‡∏•‡∏∞ Table ‡∏ó‡∏µ‡πà‡∏ã‡πà‡∏≠‡∏ô‡πÑ‡∏ß‡πâ)
    for element in soup(["script", "style", "head", "meta", "link", "noscript"]):
        element.decompose()
        
    # (Optional) ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Base64/Binary ‡∏¢‡∏≤‡∏ß‡πÜ ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏´‡∏•‡∏∏‡∏î‡∏£‡∏≠‡∏î‡∏°‡∏≤
    # (‡πÄ‡∏ä‡πà‡∏ô ‡∏ñ‡πâ‡∏≤‡∏°‡∏±‡∏ô‡πÑ‡∏°‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô tag graphic ‡πÅ‡∏ï‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô div)
    # ‡πÅ‡∏ï‡πà‡∏õ‡∏Å‡∏ï‡∏¥ Step 1 ‡∏à‡∏∞‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ 99% ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏£‡∏±‡∏ö

    # --- Step 3: Extract Text ---
    text = soup.get_text(separator=" ", strip=True)
    # ‡∏•‡∏ö‡∏Ñ‡∏≥‡∏û‡∏ß‡∏Å us-gaap:AbcdefMember ‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ‡πÄ‡∏•‡∏¢
    text = re.sub(r'\b[a-z0-9]+:[A-Za-z0-9_]+Member\b', '', text)
    text = re.sub(r'\b[a-z0-9]+:[A-Za-z0-9_]+\b', '', text)
    
    # ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏´‡∏ô‡πâ‡∏≤‡∏õ‡∏Å‡πÅ‡∏•‡∏∞‡∏™‡∏≤‡∏£‡∏ö‡∏±‡∏ç ---
    text = crop_document_content(text)
    # ‡∏•‡∏ö Whitespace ‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô
    text = " ".join(text.split())
    
    return text

# --- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ ---
def crop_document_content(text: str) -> str:
    """
    ‡∏ï‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏±‡∏ß (Cover/TOC) ‡πÅ‡∏•‡∏∞‡∏™‡πà‡∏ß‡∏ô‡∏ó‡πâ‡∏≤‡∏¢ (Exhibits/Signatures) ‡∏≠‡∏≠‡∏Å
    ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å (Item 1 - Item 15)
    """
    
    # 1. ‡∏´‡∏≤‡∏à‡∏∏‡∏î‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô: "Item 1. Business"
    # (Regex: ‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ Item ‡∏ï‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏•‡∏Ç 1 ‡∏à‡∏∏‡∏î ‡πÅ‡∏•‡∏∞ Business ‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏™‡∏ô‡πÉ‡∏à‡∏ï‡∏±‡∏ß‡∏û‡∏¥‡∏°‡∏û‡πå‡πÄ‡∏•‡πá‡∏Å‡πÉ‡∏´‡∏ç‡πà)
    # ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏™‡∏≤‡∏£‡∏ö‡∏±‡∏ç‡∏°‡∏±‡∏Å‡∏à‡∏∞‡∏°‡∏µ Item 1. Business ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ô ‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ Logic ‡∏ß‡πà‡∏≤
    # "‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠‡∏´‡∏•‡∏≤‡∏¢‡∏≠‡∏±‡∏ô ‡πÉ‡∏´‡πâ‡πÄ‡∏≠‡∏≤‡∏≠‡∏±‡∏ô‡∏ó‡∏µ‡πà 2 (‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡∏à‡∏£‡∏¥‡∏á)" ‡∏´‡∏£‡∏∑‡∏≠ "‡πÄ‡∏≠‡∏≤‡∏≠‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏•‡∏∂‡∏Å‡∏Å‡∏ß‡πà‡∏≤"
    
    start_pattern = r"Item\s+1\.?\s+Business"
    matches = list(re.finditer(start_pattern, text, re.IGNORECASE))
    
    start_index = 0
    if len(matches) >= 2:
        # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 1 (‡πÅ‡∏õ‡∏•‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏™‡∏≤‡∏£‡∏ö‡∏±‡∏ç) -> ‡πÉ‡∏´‡πâ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏ô‡∏ó‡∏µ‡πà 2 (‡∏≠‡∏±‡∏ô‡∏ó‡∏µ‡πà 1 ‡∏Ñ‡∏∑‡∏≠‡∏™‡∏≤‡∏£‡∏ö‡∏±‡∏ç)
        start_index = matches[1].start()
        print("‚úÇÔ∏è Cropping: Found TOC, starting at 2nd occurrence of Item 1.")
    elif len(matches) == 1:
        # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠‡∏≠‡∏±‡∏ô‡πÄ‡∏î‡∏µ‡∏¢‡∏ß -> ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡∏£‡∏á‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏•‡∏¢
        start_index = matches[0].start()
        print("‚úÇÔ∏è Cropping: Found Start of Item 1.")
    else:
        print("‚ö†Ô∏è Cropping: 'Item 1. Business' not found. Using full text.")

    # 2. ‡∏´‡∏≤‡∏à‡∏∏‡∏î‡∏à‡∏ö: "Item 15. Exhibits" ‡∏´‡∏£‡∏∑‡∏≠ "Signatures"
    # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏ó‡πâ‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏£‡∏Å‡∏£‡∏∏‡∏á‡∏£‡∏±‡∏á‡∏≠‡∏≠‡∏Å
    end_pattern = r"(Item\s+15\.?\s+Exhibits|SIGNATURES)"
    end_match = re.search(end_pattern, text[start_index:], re.IGNORECASE)
    
    end_index = len(text)
    if end_match:
        # ‡∏ï‡πâ‡∏≠‡∏á‡∏ö‡∏ß‡∏Å start_index ‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏£‡∏≤ search ‡πÉ‡∏ô substring
        end_index = start_index + end_match.start()
        print("‚úÇÔ∏è Cropping: Found End of document (Item 15/Signatures).")
        
    # 3. ‡∏ï‡∏±‡∏î‡∏â‡∏±‡∏ö! üó°Ô∏è
    cropped_text = text[start_index:end_index]
    
    # ‡∏Å‡∏±‡∏ô‡πÄ‡∏´‡∏ô‡∏µ‡∏¢‡∏ß: ‡∏ñ‡πâ‡∏≤‡∏ï‡∏±‡∏î‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏™‡∏±‡πâ‡∏ô‡∏à‡∏∏‡πä‡∏î‡∏à‡∏π‡πã (‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î) ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏°‡πÑ‡∏õ
    if len(cropped_text) < 1000:
        print("‚ö†Ô∏è Cropping result too short. Reverting to full text.")
        return text
        
    return cropped_text

async def fetch_and_process_10k(user_id: int, ticker: str, amount: int = 1):
    ticker = ticker.upper()
    log.info(f"üîç Fetching 10-K for {ticker}...")
    dl = Downloader("Investi-Graph", settings.SEC_API_EMAIL, TEMP_SEC_DIR)

    try:
        dl.get("10-K", ticker, limit=amount)
        
        search_path = os.path.join(TEMP_SEC_DIR, "sec-edgar-filings", ticker, "10-K", "*", "*.txt")
        files = glob.glob(search_path)
        
        if not files:
            log.error(f"No 10-K found for {ticker}")
            return

        file_path = files[0]
        log.info(f"üìÇ Found file: {file_path}")

        # 3. ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå
        with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
            raw_content = f.read()
            
        # --- 4. Clean HTML ‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ---
        log.info("üßπ Cleaning HTML content...")
        clean_text = clean_html_content(raw_content)
        log.info(f"Cleaned text length: {len(clean_text)}")
        
        # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô bytes
        content_bytes = clean_text.encode("utf-8")
        filename = f"{ticker}_10K_Report.txt"

        # 5. ‡∏™‡πà‡∏á‡∏ï‡πà‡∏≠‡πÉ‡∏´‡πâ Pipeline (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°)
        async with SessionLocal() as db:
            db_doc = await crud.create_document(db=db, filename=filename, owner_id=user_id)
            
            await processing.save_extract_chunk_and_embed(
                document_id=db_doc.id,
                filename=filename,
                content_type="text/plain", # ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô Text ‡∏•‡πâ‡∏ß‡∏ô‡πÅ‡∏•‡πâ‡∏ß
                content=content_bytes
            )

        log.info(f"‚úÖ SEC Fetch & Process Complete for {ticker}")

    except Exception as e:
        log.error(f"‚ùå Error fetching SEC data: {e}")
    
    finally:
        if os.path.exists(os.path.join(TEMP_SEC_DIR, "sec-edgar-filings", ticker)):
             shutil.rmtree(os.path.join(TEMP_SEC_DIR, "sec-edgar-filings", ticker))